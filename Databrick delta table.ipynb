{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09a9efbd-0708-44d2-8f1e-d1a0dd22765b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"DeltaExample\").getOrCreate()\n",
    "\n",
    "# Define data\n",
    "data = [\n",
    "    (1, \"Alice\", 29),\n",
    "    (2, \"Bob\", 35),\n",
    "    (3, \"Charlie\", 40)\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"name\", \"age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Save as Delta Table\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/delta/raw_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7daa4c0c-f044-4abb-b576-db94175b2470",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Load existing Delta Table\n",
    "delta_table = DeltaTable.forPath(spark, \"/mnt/delta/raw_data\")\n",
    "\n",
    "# Perform an incremental update\n",
    "new_data = spark.createDataFrame([(4, \"David\", 25)], [\"id\", \"name\", \"age\"])\n",
    "\n",
    "# Merge logic: Insert new records\n",
    "delta_table.alias(\"old\").merge(\n",
    "    new_data.alias(\"new\"),\n",
    "    \"old.id = new.id\"\n",
    ").whenNotMatchedInsertAll().execute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c2c5b76-44f0-4e9d-b539-39ef1adc7d0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[3]: <pyspark.sql.streaming.query.StreamingQuery at 0x7f2735bdbf40>"
     ]
    }
   ],
   "source": [
    "# Simulate streaming data\n",
    "streaming_df = spark.readStream.format(\"delta\").load(\"/mnt/delta/raw_data\")\n",
    "\n",
    "# Write streaming data to a new Delta Table\n",
    "streaming_df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/mnt/delta/checkpoint\") \\\n",
    "    .start(\"/mnt/delta/processed_data\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Databrick delta table",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
